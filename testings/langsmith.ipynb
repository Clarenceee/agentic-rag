{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05af73e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clarencechan/Documents/agentic-rag/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "import langsmith\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "from langchain.smith import RunEvalConfig\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/clarencechan/Documents/agentic-rag/process/.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057efb53",
   "metadata": {},
   "source": [
    "#### URL - https://levelup.gitconnected.com/implementing-12-ai-agent-evaluation-techniques-using-langsmith-507d5bf5c0aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a847212",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-5-mini'\n",
    "client = langsmith.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb227169",
   "metadata": {},
   "source": [
    "#### Exact matching based evaluation\n",
    "- Typiecall used for RAG or AI Agent tasks when the exepcted output is deterministic\n",
    "- Fact-based QA\n",
    "- Close-ended question\n",
    "- Tool usage outputs\n",
    "- Structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28267bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['17e3104e-cad3-43ed-841a-a74f29eef1a7',\n",
       "  'b8394435-c9f2-4504-9aea-58af001648c3'],\n",
       " 'count': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'test-dataset'\n",
    "ds = client.read_dataset(dataset_name=dataset_name)\n",
    "\n",
    "# ds = client.create_dataset(\n",
    "#     dataset_name=dataset_name,\n",
    "#     description='Test dataset for tutorial & learning purposes.',\n",
    "# )\n",
    "\n",
    "# client.create_examples(\n",
    "#     inputs=[\n",
    "#         {\n",
    "#             \"prompt_template\": \"State the year of the declaration of independence. Respond with just the year in digits, nothing else\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"prompt_template\": \"What's the average speed of an unladen swallow?\"\n",
    "#         },\n",
    "#     ],\n",
    "#     # List of corresponding outputs.\n",
    "#     outputs=[\n",
    "#         {\"output\": \"1776\"},\n",
    "#         {\"output\": \"5\"}\n",
    "#     ],\n",
    "#     dataset_id=ds.id,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36b25684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our \"system under test\". It takes an input dictionary,\n",
    "# invokes the specified ChatOpenAI model, and returns the output in a dictionary.\n",
    "def predict_result(input_: dict) -> dict:\n",
    "    # The input dictionary for this function will have the key \"prompt_template\"\n",
    "    # which matches the key we defined in our dataset's inputs.\n",
    "    prompt = input_[\"prompt_template\"]\n",
    "    \n",
    "    # Initialize and call the model\n",
    "    response = ChatOpenAI(model=model, temperature=0).invoke(prompt)\n",
    "    \n",
    "    # The output key \"output\" matches the key in our dataset's outputs for comparison.\n",
    "    return {\"output\": response.content}\n",
    "\n",
    "# Build Evaluators\n",
    "# Built-in exact_match evaluator\n",
    "# Custom compare_label evaluator using @run_evaluator\n",
    "# The @run_evaluator decorator registers this function as a custom evaluator\n",
    "@run_evaluator\n",
    "def compare_label(run, example) -> EvaluationResult:\n",
    "    \"\"\"\n",
    "    A custom evaluator that checks for an exact match.\n",
    "    \n",
    "    Args:\n",
    "        run: The LangSmith run object, which contains the model's outputs.\n",
    "        example: The LangSmith example object, which contains the reference data.\n",
    "    \n",
    "    Returns:\n",
    "        An EvaluationResult object with a key and a score.\n",
    "    \"\"\"\n",
    "    # Get the model's prediction from the run's outputs dictionary.\n",
    "    # The key 'output' must match what our `predict_result` function returns.\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    \n",
    "    # Get the reference answer from the example's outputs dictionary.\n",
    "    # The key 'output' must match what we defined in our dataset.\n",
    "    target = example.outputs.get(\"output\") or \"\"\n",
    "    \n",
    "    # Perform the comparison.\n",
    "    match = prediction == target\n",
    "    \n",
    "    # Return the result. The key is how the score will be named in the results.\n",
    "    # The score for exact match is typically binary (1 for a match, 0 for a mismatch).\n",
    "    return EvaluationResult(key=\"matches_label\", score=int(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5d75e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'damp-position-49' at:\n",
      "https://smith.langchain.com/o/447a3e6d-e186-494e-9c8e-c4650326fd0d/datasets/98e8b24c-e8f7-4fc4-b539-64e59cc3348e/compare?selectedSessions=1300f7ea-cd5d-4f65-8939-8a8d399b9072\n",
      "\n",
      "View all tests for Dataset test-dataset at:\n",
      "https://smith.langchain.com/o/447a3e6d-e186-494e-9c8e-c4650326fd0d/datasets/98e8b24c-e8f7-4fc4-b539-64e59cc3348e\n",
      "[------------------------------------------------->] 2/2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.exact_match</th>\n",
       "      <th>feedback.matches_label</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42719130-216b-4df3-b198-3617881b8a32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.866100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.975639</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.762006</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.814053</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.866100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.918148</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.970195</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.exact_match  feedback.matches_label error  execution_time  \\\n",
       "count               2.000000                2.000000     0        2.000000   \n",
       "unique                   NaN                     NaN     0             NaN   \n",
       "top                      NaN                     NaN   NaN             NaN   \n",
       "freq                     NaN                     NaN   NaN             NaN   \n",
       "mean                0.500000                0.500000   NaN        6.866100   \n",
       "std                 0.707107                0.707107   NaN        2.975639   \n",
       "min                 0.000000                0.000000   NaN        4.762006   \n",
       "25%                 0.250000                0.250000   NaN        5.814053   \n",
       "50%                 0.500000                0.500000   NaN        6.866100   \n",
       "75%                 0.750000                0.750000   NaN        7.918148   \n",
       "max                 1.000000                1.000000   NaN        8.970195   \n",
       "\n",
       "                                      run_id  \n",
       "count                                      2  \n",
       "unique                                     2  \n",
       "top     42719130-216b-4df3-b198-3617881b8a32  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'damp-position-49',\n",
       " 'results': {'17e3104e-cad3-43ed-841a-a74f29eef1a7': {'input': {'prompt_template': 'State the year of the declaration of independence. Respond with just the year in digits, nothing else'},\n",
       "   'feedback': [EvaluationResult(key='exact_match', score=1, value=None, comment=None, correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('904f4139-2d76-44e1-a455-7f8e85e085ca'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None),\n",
       "    EvaluationResult(key='matches_label', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('70ab3418-d3e2-45af-be57-64cbc6136fbc'), target_run_id=None, extra=None)],\n",
       "   'execution_time': 4.762006,\n",
       "   'run_id': '42719130-216b-4df3-b198-3617881b8a32',\n",
       "   'output': {'output': '1776'},\n",
       "   'reference': {'output': '1776'}},\n",
       "  'b8394435-c9f2-4504-9aea-58af001648c3': {'input': {'prompt_template': \"What's the average speed of an unladen swallow?\"},\n",
       "   'feedback': [EvaluationResult(key='exact_match', score=0, value=None, comment=None, correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('415f83d5-1045-4cd4-a1e1-cb0580252b1d'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None),\n",
       "    EvaluationResult(key='matches_label', score=0, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3867e8ac-8bcd-48f4-9478-47cc78815999'), target_run_id=None, extra=None)],\n",
       "   'execution_time': 8.970195,\n",
       "   'run_id': 'ba509f72-e212-4798-afd4-ba231dc62a7e',\n",
       "   'output': {'output': 'Do you mean an African or a European swallow?\\n\\nIf you mean the European swallow (barn swallow, Hirundo rustica), a commonly quoted cruise speed is about 11 m/s — roughly 24 mph or 40 km/h. (Estimates vary with flight mode, wind, and species; different African swallow species can have different speeds.)'},\n",
       "   'reference': {'output': '5'}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This defines the configuration for our evaluation run.\n",
    "# Specoify both custom and built in evaluaators - every model run will be scored by both\n",
    "eval_config = RunEvalConfig(\n",
    "    # We can specify built-in evaluators by their string names.\n",
    "    evaluators=[\"exact_match\"], \n",
    "    \n",
    "    # We pass our custom evaluator function directly in a list.\n",
    "    custom_evaluators=[compare_label],\n",
    ")\n",
    "\n",
    "# This command triggers the evaluation.\n",
    "# It will run the `predict_result` function for each example in the dataset\n",
    "# and then score the results using the evaluators in `eval_config`.\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=predict_result,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True, # This will print the progress bar and links\n",
    "    project_metadata={\"version\": \"1.0.1\", \"model\": model}, # Optional metadata for the project\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb23d27",
   "metadata": {},
   "source": [
    "#### Unstructured Q&A Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c776aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main difference is that our GT answers are now reference points of correctness not templates for an exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "741790e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ID = 98e8b24c-e8f7-4fc4-b539-64e59cc3348e\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset in LangSmith\n",
    "client = langsmith.Client()\n",
    "try: \n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Q&A dataset about LangSmith documentation.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "print(f\"Dataset ID = {dataset.id}\")\n",
    "\n",
    "# These are our question-and-answer examples. The answers serve as 'ground truth'.\n",
    "qa_examples = [\n",
    "    (\n",
    "        \"What is LangChain?\",\n",
    "        \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How might I query for all runs in a project?\",\n",
    "        \"You can use client.list_runs(project_name='my-project-name') in Python, or client.ListRuns({projectName: 'my-project-name'}) in TypeScript.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What's a langsmith dataset?\",\n",
    "        \"A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How do I move my project between organizations?\",\n",
    "        \"LangSmith doesn't directly support moving projects between organizations.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Add the examples to our dataset\n",
    "# The input key is 'question' and the output key is 'answer'.\n",
    "# These keys must match what our RAG chain expects and produces.\n",
    "for question, answer in qa_examples:\n",
    "    client.create_example(\n",
    "        inputs={\"question\": question},\n",
    "        outputs={\"answer\": answer},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_result(input_: dict) -> dict:\n",
    "    # The input dictionary for this function will have the key \"prompt_template\"\n",
    "    # which matches the key we defined in our dataset's inputs.\n",
    "    prompt = input_.get(\"question\") or input_.get('prompt_template')\n",
    "    \n",
    "    # Initialize and call the model\n",
    "    response = ChatOpenAI(model=model, temperature=0).invoke(prompt)\n",
    "    \n",
    "    # The output key \"output\" matches the key in our dataset's outputs for comparison.\n",
    "    return {\"output\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "056d6d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'extraneous-shoe-50' at:\n",
      "https://smith.langchain.com/o/447a3e6d-e186-494e-9c8e-c4650326fd0d/datasets/98e8b24c-e8f7-4fc4-b539-64e59cc3348e/compare?selectedSessions=4fa2aecc-4a93-4c7e-b621-770130ba0ef7\n",
      "\n",
      "View all tests for Dataset test-dataset at:\n",
      "https://smith.langchain.com/o/447a3e6d-e186-494e-9c8e-c4650326fd0d/datasets/98e8b24c-e8f7-4fc4-b539-64e59cc3348e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 17e3104e-cad3-43ed-841a-a74f29eef1a7 with inputs {'prompt_template': 'State the year of the declaration of independence. Respond with just the year in digits, nothing else'}\n",
      "Error Type: KeyError, Message: 'question'\n",
      "Chain failed for example b8394435-c9f2-4504-9aea-58af001648c3 with inputs {'prompt_template': \"What's the average speed of an unladen swallow?\"}\n",
      "Error Type: KeyError, Message: 'question'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 6/6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>'question'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d02e80c5-f9de-4a8c-b1a0-37a5073405dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.364198</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.018547</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.260110</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.210899</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.446117</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.961527</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.correctness       error  execution_time  \\\n",
       "count                4.00000           2        6.000000   \n",
       "unique                   NaN           2             NaN   \n",
       "top                      NaN  'question'             NaN   \n",
       "freq                     NaN           1             NaN   \n",
       "mean                 0.50000         NaN       18.364198   \n",
       "std                  0.57735         NaN       15.018547   \n",
       "min                  0.00000         NaN        0.000826   \n",
       "25%                  0.00000         NaN        5.260110   \n",
       "50%                  0.50000         NaN       22.210899   \n",
       "75%                  1.00000         NaN       30.446117   \n",
       "max                  1.00000         NaN       32.961527   \n",
       "\n",
       "                                      run_id  \n",
       "count                                      6  \n",
       "unique                                     6  \n",
       "top     d02e80c5-f9de-4a8c-b1a0-37a5073405dd  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'extraneous-shoe-50',\n",
       " 'results': {'f1e31e1f-a0a5-400b-9572-1647b90bcb0b': {'input': {'question': 'How do I move my project between organizations?'},\n",
       "   'feedback': [EvaluationResult(key='correctness', score=0, value='INCORRECT', comment='INCORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('07f82966-2b2e-4af3-aa54-297bbc050219'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 32.961527,\n",
       "   'run_id': 'd02e80c5-f9de-4a8c-b1a0-37a5073405dd',\n",
       "   'output': {'output': 'Which service/platform are you using (GitHub, GitLab, Google Cloud, Azure DevOps, Bitbucket, Jira, etc.)? Exact steps depend on that — I can give a step‑by‑step for the product you use.\\n\\nMeanwhile, here’s a short general checklist plus quick instructions for several common platforms.\\n\\nGeneral checklist before you move anything\\n- Confirm you have the required permissions in both source and target organizations.\\n- Back up the project/repo/data (export, clone, database/export utilities).\\n- Note down and plan to reconfigure: CI/CD, secrets, service accounts, webhooks, OAuth apps, DNS/custom domains, billing.\\n- Communicate downtime/expectations with team and users.\\n- Test the move in a sandbox if possible and validate everything after transfer.\\n\\nQuick platform-specific notes\\n\\nGitHub (repository -> organization)\\n- Permissions: you must be repo owner; target org must allow repo creation or you must be an org owner.\\n- Steps: Repo -> Settings -> Transfer (Owner) -> enter target organization name and confirm repository name.\\n- Post-move: update Actions secrets, OAuth apps, webhooks, GitHub Pages config, and any protected-branch or team permissions.\\n\\nGitLab (project -> group/namespace)\\n- Permissions: typically Maintainer on the project and permission to create projects in the target group.\\n- Steps: Project -> Settings -> General -> Advanced -> Transfer project -> enter target group/namespace and confirm.\\n- For cross-instance moves use project export/import (Project -> Settings -> General -> Export/Import).\\n\\nGoogle Cloud Platform (project -> organization/folder)\\n- Permissions: resourcemanager.projects.move on the project and appropriate roles in the destination organization (and possibly billing account permissions).\\n- Steps (Console): IAM & Admin -> Manage resources -> select project -> Move -> choose destination organization/folder -> confirm.\\n- Check APIs, service accounts, billing linkage, and organization policies that may block move.\\n\\nAzure DevOps (project -> organization)\\n- Note: there is no simple one-click move between organizations.\\n- Typical approach: migrate content piecemeal — mirror git repos (push to new remote), use migration tools or Azure DevOps Migration Tools for work items, recreate pipelines, move artifacts separately.\\n- For large/complex moves consider Microsoft support or a migration partner.\\n\\nBitbucket Cloud\\n- Repo transfer: Repository settings -> Transfer repository -> enter target workspace and confirm.\\n- For Bitbucket Server/Data Center, an admin can move repos between projects via repository administration.\\n\\nJira\\n- Moving a project between Jira sites/instances usually requires project export/import or using Atlassian Cloud site migration tools; fairly involved for related attachments, boards, workflows.\\n\\nIf you tell me which platform and whether you want to move a single repo/project or an entire organization of multiple projects, I’ll give exact, step‑by‑step instructions and a migration checklist tailored to your situation.'},\n",
       "   'reference': {'answer': \"LangSmith doesn't directly support moving projects between organizations.\"}},\n",
       "  '8b71a191-10d6-454d-bf87-d31ba5a7be80': {'input': {'question': \"What's a langsmith dataset?\"},\n",
       "   'feedback': [EvaluationResult(key='correctness', score=1, value='CORRECT', comment='CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a0c06c83-b683-40fc-82b6-42d5c380faad'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 32.791069,\n",
       "   'run_id': '7ceecaa7-1ee6-4cee-8fbc-f7c437da20d7',\n",
       "   'output': {'output': 'A LangSmith dataset is a named collection of example items you store in the LangSmith platform to drive evaluation, testing, human annotation, and (optionally) fine‑tuning workflows for LLMs. It’s how you give LangSmith the inputs, expected outputs (references/labels), and metadata it needs to run automatic or human evaluations and keep traceable test suites.\\n\\nKey points\\n- Purpose: feed examples for evaluations, regressions tests, human review, and as labeled data for training workflows.\\n- Format: typically a JSON/JSONL collection of examples where each example has an id, the model input (prompt), one or more expected outputs (reference/labels), and optional metadata/annotations/split (train/val/test).\\n- Uses: automated scoring (exact match, BLEU/ROUGE, custom scorers), side‑by‑side comparisons across model versions, human labeling, dataset slicing/filtering by metadata, and track results over time in the LangSmith UI.\\n- How to load: upload via the LangSmith web UI or programmatically with the LangSmith SDK (Python/JS) by sending a list of example objects or a JSONL file.\\n\\nTypical example record (JSONL)\\nEach line is one JSON object. Field names vary by team, but common fields:\\n{\"id\": \"ex-001\",\\n \"input\": \"Summarize: The quick brown fox jumps over the lazy dog.\",\\n \"ideal\": \"A brief summary about a fox jumping over a dog.\",\\n \"split\": \"eval\",\\n \"metadata\": {\"topic\": \"animals\", \"source\": \"synthetic\"}}\\n\\nNotes and best practices\\n- Provide clear expected outputs. For open‑ended tasks include multiple references or mark that an exact match is not expected and use a semantic metric or human review.\\n- Include metadata (source, difficulty, tags) to allow slicing and targeted evaluations.\\n- Use splits (train/val/test) if you plan to fine‑tune or to separate regression tests.\\n- Avoid releasing PII/secret content.\\n- Keep examples representative and diverse for meaningful evaluations.\\n\\nIf you want, tell me the task (classification, summarization, QA, code generation) and I can give a ready‑to‑upload JSONL template and recommended metadata fields for that use case.'},\n",
       "   'reference': {'answer': 'A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.'}},\n",
       "  '72bec99f-2e63-4f44-a53c-dfa1b4bfe281': {'input': {'question': 'How might I query for all runs in a project?'},\n",
       "   'feedback': [EvaluationResult(key='correctness', score=0, value='INCORRECT', comment='INCORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7a91baac-2538-4301-b2eb-725d970ae766'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 23.41126,\n",
       "   'run_id': '7920b33f-df40-4e8f-8f35-e171a6719b26',\n",
       "   'output': {'output': 'Which system or service are you using for \"runs\" (e.g., MLflow, Weights & Biases, GitHub Actions, GitLab CI, Jenkins, Azure DevOps, Databricks, etc.)? I can give a precise query for that.\\n\\nMeanwhile, here are common patterns and example queries for several popular systems:\\n\\n- MLflow (REST)\\n  - Search runs for an experiment (POST):\\n    curl -X POST -H \"Content-Type: application/json\" \\\\\\n      -d \\'{\"experiment_ids\":[\"0\"],\"max_results\":100,\"filter\":\"\"}\\' \\\\\\n      https://<mlflow-host>/api/2.0/mlflow/runs/search\\n  - Python SDK:\\n    from mlflow.tracking import MlflowClient\\n    client = MlflowClient()\\n    runs = client.search_runs(experiment_ids=[\"0\"], filter_string=\"\", max_results=100)\\n\\n- Weights & Biases (W&B)\\n  - Python:\\n    import wandb\\n    api = wandb.Api()\\n    runs = api.runs(\"entity/project\")            # yields iterator of runs\\n  - REST:\\n    GET https://api.wandb.ai/projects/{entity}/{project}/runs (requires auth)\\n\\n- GitHub Actions (workflow runs)\\n  - List workflow runs:\\n    curl -H \"Authorization: token <TOKEN>\" \\\\\\n      \"https://api.github.com/repos/{owner}/{repo}/actions/runs?per_page=100\"\\n\\n- GitLab CI (pipelines)\\n  - List pipelines for a project:\\n    curl --header \"PRIVATE-TOKEN: <token>\" \\\\\\n      \"https://gitlab.example.com/api/v4/projects/:id/pipelines?per_page=100\"\\n\\n- Jenkins\\n  - Project/job builds:\\n    GET http://jenkins.example.com/job/{jobName}/api/json?tree=builds[number,result,timestamp,id]\\n\\n- Azure DevOps\\n  - List builds:\\n    GET https://dev.azure.com/{org}/{project}/_apis/build/builds?api-version=6.0\\n\\nGeneral tips\\n- Authentication: include an API token or appropriate auth header.\\n- Pagination: most APIs return pages; use page, per_page, next/continuation token or cursor.\\n- Filtering & fields: use filter/query parameters or request specific fields to reduce payload.\\n- Rate limits: be mindful of API rate limits; use backoff or small page sizes if needed.\\n\\nTell me which service you want to query and whether you prefer curl, Python, or GraphQL and I’ll give an exact snippet.'},\n",
       "   'reference': {'answer': \"You can use client.list_runs(project_name='my-project-name') in Python, or client.ListRuns({projectName: 'my-project-name'}) in TypeScript.\"}},\n",
       "  '9442c405-309a-4793-af1b-d61405f57098': {'input': {'question': 'What is LangChain?'},\n",
       "   'feedback': [EvaluationResult(key='correctness', score=1, value='CORRECT', comment='CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8f3aa42f-2e44-4951-8e2b-f01d9c03f33b'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 21.010538,\n",
       "   'run_id': 'd944d54e-50fc-4a0e-a699-714547c124d1',\n",
       "   'output': {'output': 'Short answer\\n- LangChain is an open-source framework for building applications that use large language models (LLMs). It provides building blocks and integrations so you can compose prompts, chain LLM calls, add memory, connect to external data (vector stores, databases, APIs), and build agents that call tools.\\n\\nWhat it solves\\n- LLMs are powerful but need orchestration to build real apps (retrieval-augmented generation, chatbots, automation pipelines). LangChain supplies the reusable components to glue models, data, tools, and application logic together.\\n\\nCore concepts\\n- Models: adapters for LLM providers (OpenAI, Anthropic, Hugging Face, etc.).\\n- Prompts & PromptTemplates: parameterized prompt management and formatting.\\n- Chains: sequences of calls (prompts, LLMs, transforms) to implement workflows.\\n- Agents & Tools: allow an LLM to decide which external tools (search, calculator, APIs, Python REPL) to call and in what order.\\n- Memory: keep conversational or stateful memory across turns.\\n- Retrievers & Vectorstores (Indexes): enable retrieval-augmented generation using embeddings and vector DBs (FAISS, Pinecone, Weaviate, Milvus, etc.).\\n- Callbacks & Logging: instrumentation for tracing, evaluation, and debugging.\\n\\nTypical uses\\n- Retrieval-augmented generation (RAG) for QA over documents.\\n- Conversational assistants with memory and background knowledge.\\n- Multi-step pipelines (summarization, extraction, transformation).\\n- Autonomous agents that use web search, code execution, or APIs.\\n- Application integration (CRM summaries, code generation, data-to-text).\\n\\nLanguages & ecosystem\\n- Available for Python and JavaScript/TypeScript, with many community and commercial integrations (model providers, vector DBs, data connectors).\\n\\nMinimal Python example (conceptual)\\nfrom langchain import OpenAI, PromptTemplate, LLMChain\\ntemplate = \"Summarize this text in one sentence:\\\\n\\\\n{text}\"\\nprompt = PromptTemplate(template=template, input_variables=[\"text\"])\\nllm = OpenAI(temperature=0)\\nchain = LLMChain(llm=llm, prompt=prompt)\\nchain.run(\"LangChain helps build LLM-powered apps by providing chains, agents, memory, and integrations.\")\\n\\nWhen to use LangChain\\n- Use it when you need structured orchestration of LLM calls, retrieval from external data, multi-step flows, or agents/tools. If you only need a single prompt to an LLM, plain API calls may be simpler.\\n\\nFurther resources\\n- Official LangChain docs and examples (search “LangChain docs”) — they show recipes for RAG, agents, memory, and deployments.'},\n",
       "   'reference': {'answer': 'LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.'}},\n",
       "  '17e3104e-cad3-43ed-841a-a74f29eef1a7': {'input': {'prompt_template': 'State the year of the declaration of independence. Respond with just the year in digits, nothing else'},\n",
       "   'feedback': [],\n",
       "   'execution_time': 0.009968,\n",
       "   'run_id': 'e0c3ba31-5dfa-48c2-a652-ae636431701d',\n",
       "   'Error': KeyError('question'),\n",
       "   'reference': {'output': '1776'}},\n",
       "  'b8394435-c9f2-4504-9aea-58af001648c3': {'input': {'prompt_template': \"What's the average speed of an unladen swallow?\"},\n",
       "   'feedback': [],\n",
       "   'execution_time': 0.000826,\n",
       "   'run_id': 'fafd2b39-cb3a-4a62-8665-f428415e16cc',\n",
       "   'Error': KeyError('question'),\n",
       "   'reference': {'output': '5'}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the evaluation to use the \"qa\" evaluator, which grades for\n",
    "# \"correctness\" based on the reference answer.\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"qa\"],\n",
    ")\n",
    "\n",
    "# Run the RAG chain over the dataset and apply the evaluator\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=predict_result,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_metadata={\"version\": \"1.0.0\", \"model\": \"gpt-5-mini\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a128eb8",
   "metadata": {},
   "source": [
    "#### Structured Data Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77da0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = langsmith.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82d13bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='Contract Extraction Eval Dataset', description=None, data_type=<DataType.kv: 'kv'>, id=UUID('f1f777fb-0b39-4a08-93d8-c098adc18756'), created_at=datetime.datetime(2025, 10, 3, 14, 54, 37, 411802, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 10, 3, 14, 54, 37, 411802, tzinfo=datetime.timezone.utc), example_count=0, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None, transformations=None, metadata=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The URL of the public dataset on LangSmith\n",
    "dataset_url = \"https://smith.langchain.com/public/08ab7912-006e-4c00-a973-0f833e74907b/d\"\n",
    "dataset_name = \"Contract Extraction Eval Dataset\"\n",
    "\n",
    "# Clone the public dataset to your own account\n",
    "client.clone_public_dataset(dataset_url, dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a05a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Define the schema for a party's address\n",
    "class Address(BaseModel):\n",
    "    street: str\n",
    "    city: str\n",
    "    state: str\n",
    "\n",
    "# Define the schema for a party in the contract\n",
    "class Party(BaseModel):\n",
    "    name: str\n",
    "    address: Address\n",
    "\n",
    "# The top-level schema for the entire contract\n",
    "class Contract(BaseModel):\n",
    "    document_title: str\n",
    "    effective_date: str\n",
    "    parties: List[Party]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf0924a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/2qwvhqqd5x38g72sq_ktmlzm0000gn/T/ipykernel_27519/258324972.py:10: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  extraction_chain = create_extraction_chain(Contract.schema(), llm)\n",
      "/var/folders/40/2qwvhqqd5x38g72sq_ktmlzm0000gn/T/ipykernel_27519/258324972.py:10: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` thatis available on ChatModels capable of tool calling.You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>. Please follow our extraction use case documentation for more guidelineson how to do information extraction with LLMs.<https://python.langchain.com/docs/use_cases/extraction/>. If you notice other issues, please provide feedback here:<https://github.com/langchain-ai/langchain/discussions/18154>\n",
      "  extraction_chain = create_extraction_chain(Contract.schema(), llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "# For this task, we'll use a powerful model capable of following complex instructions.\n",
    "# Note: You can swap this with an equivalent OpenAI model.\n",
    "llm = ChatAnthropic(model=\"claude-2.1\", temperature=0, max_tokens=4000)\n",
    "\n",
    "# Create the extraction chain, providing the schema and the LLM.\n",
    "extraction_chain = create_extraction_chain(Contract.schema(), llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7535dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation configuration specifies our JSON-aware evaluator.\n",
    "# The 'json_edit_distance' evaluator compares the structure and content of two JSON objects.\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"json_edit_distance\"]\n",
    ")\n",
    "\n",
    "# # Run the evaluation\n",
    "# client.run_on_dataset(\n",
    "#     dataset_name='Contract Extraction Eval Dataset',\n",
    "#     llm_or_chain_factory=extraction_chain,\n",
    "#     evaluation=eval_config,\n",
    "#     # The input key in our dataset is 'context', which we map to the chain's 'input' key.\n",
    "#     input_mapper=lambda x: {\"input\": x[\"context\"]},\n",
    "#     # The output from the chain is a dict {'text': [...]}, we care about the 'text' value.\n",
    "#     output_mapper=lambda x: x['text'],\n",
    "#     verbose=True,\n",
    "#     project_metadata={\"version\": \"1.0.0\", \"model\": \"claude-2.1\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147991b",
   "metadata": {},
   "source": [
    "#### Dynamic Ground Truth\n",
    "- E.g. based on a live database, an inventory system, or a constantly updating API, how can you create a reliable test set?\n",
    "- use indirection, instead of storing the static answer, store a reference or a query that can be executed at the time of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977de133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['8bf9e5c9-bf6c-4b91-bf0c-8498073d91c9',\n",
       "  '81327d10-4780-4ebf-a87c-0a8177f7292c',\n",
       "  '51abab2f-a113-4e66-b768-0b7c5c5c53da',\n",
       "  'a36f07a4-9b8c-43ff-9fc7-1caae8378fb9',\n",
       "  'ae57634f-a538-4fdd-9535-46492887a870'],\n",
       " 'count': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our list of questions and the corresponding pandas code to find the answer.\n",
    "questions_with_references = [\n",
    "    (\"How many passengers were on the Titanic?\", \"len(df)\"),\n",
    "    (\"How many passengers survived?\", \"df['Survived'].sum()\"),\n",
    "    (\"What was the average age of the passengers?\", \"df['Age'].mean()\"),\n",
    "    (\"How many male and female passengers were there?\", \"df['Sex'].value_counts()\"),\n",
    "    (\"What was the average fare paid for the tickets?\", \"df['Fare'].mean()\"),\n",
    "]\n",
    "\n",
    "# Create a unique dataset name\n",
    "dataset_name = \"Dynamic Titanic QA\"\n",
    "\n",
    "client = langsmith.Client()\n",
    "# Create the dataset in LangSmith\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA over the Titanic dataset with dynamic references.\",\n",
    ")\n",
    "\n",
    "# Populate the dataset. The input is the question, and the output is the code.\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q, r in questions_with_references],\n",
    "    outputs=[{\"reference_code\": r} for q, r in questions_with_references],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acdfb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset from a URL\n",
    "titanic_url = \"https://raw.githubusercontent.com/jorisvandenbossche/pandas-tutorial/master/data/titanic.csv\"\n",
    "df = pd.read_csv(titanic_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11eb7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM for the agent\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# This function creates and invokes the agent on the current state of `df`\n",
    "def predict_pandas_agent(inputs: dict):\n",
    "    # The agent is created with the current `df`\n",
    "    agent = create_pandas_dataframe_agent(agent_type=\"openai-tools\", llm=llm, df=df)\n",
    "    return agent.invoke({\"input\": inputs[\"question\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90bb2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain.evaluation.criteria.eval_chain import LabeledCriteriaEvalChain\n",
    "\n",
    "class DynamicReferenceEvaluator(LabeledCriteriaEvalChain):\n",
    "    def _get_eval_input(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        reference: Optional[str],\n",
    "        input: Optional[str],\n",
    "    ) -> dict:\n",
    "        # Get the standard input dictionary from the parent class\n",
    "        eval_input = super()._get_eval_input(prediction, reference, input)\n",
    "\n",
    "        # 'reference' here is our code snippet, e.g., \"len(df)\"\n",
    "        # We execute it to get the live ground truth value.\n",
    "        # WARNING: Using `eval` can be risky. Only run trusted code.\n",
    "        live_ground_truth = eval(eval_input[\"reference\"])\n",
    "        \n",
    "        # Replace the code snippet with the actual live answer\n",
    "        eval_input[\"reference\"] = str(live_ground_truth)\n",
    "        \n",
    "        return eval_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "935a20d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/2qwvhqqd5x38g72sq_ktmlzm0000gn/T/ipykernel_27519/1330203278.py:18: LangChainDeprecationWarning: The following arguments are deprecated and will be removed in a future release: dict_keys(['max_concurrency']).\n",
      "  client.run_on_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'dear-click-47' at:\n",
      "https://smith.langchain.com/o/447a3e6d-e186-494e-9c8e-c4650326fd0d/datasets/c983fc8c-d1a2-4d58-98a5-9ab775f64b0c/compare?selectedSessions=d7f19481-0e8d-4e2e-892b-c026606606a8\n",
      "\n",
      "View all tests for Dataset Dynamic Titanic QA at:\n",
      "https://smith.langchain.com/o/447a3e6d-e186-494e-9c8e-c4650326fd0d/datasets/c983fc8c-d1a2-4d58-98a5-9ab775f64b0c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 81327d10-4780-4ebf-a87c-0a8177f7292c with inputs {'question': 'How many passengers survived?'}\n",
      "Error Type: NameError, Message: name 'create_pandas_dataframe_agent' is not defined\n",
      "Chain failed for example 51abab2f-a113-4e66-b768-0b7c5c5c53da with inputs {'question': 'What was the average age of the passengers?'}\n",
      "Error Type: NameError, Message: name 'create_pandas_dataframe_agent' is not defined\n",
      "Chain failed for example 8bf9e5c9-bf6c-4b91-bf0c-8498073d91c9 with inputs {'question': 'How many passengers were on the Titanic?'}\n",
      "Error Type: NameError, Message: name 'create_pandas_dataframe_agent' is not defined\n",
      "Chain failed for example a36f07a4-9b8c-43ff-9fc7-1caae8378fb9 with inputs {'question': 'How many male and female passengers were there?'}\n",
      "Error Type: NameError, Message: name 'create_pandas_dataframe_agent' is not defined\n",
      "Chain failed for example ae57634f-a538-4fdd-9535-46492887a870 with inputs {'question': 'What was the average fare paid for the tickets?'}\n",
      "Error Type: NameError, Message: name 'create_pandas_dataframe_agent' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 5/5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'dear-click-47',\n",
       " 'results': {'51abab2f-a113-4e66-b768-0b7c5c5c53da': {'input': {'question': 'What was the average age of the passengers?'},\n",
       "   'feedback': [],\n",
       "   'execution_time': 0.014431,\n",
       "   'run_id': '0bf062dc-b6f3-4450-a422-48e3d5093f2b',\n",
       "   'Error': NameError(\"name 'create_pandas_dataframe_agent' is not defined\"),\n",
       "   'reference': {'reference_code': \"df['Age'].mean()\"}},\n",
       "  '81327d10-4780-4ebf-a87c-0a8177f7292c': {'input': {'question': 'How many passengers survived?'},\n",
       "   'feedback': [],\n",
       "   'execution_time': 0.014074,\n",
       "   'run_id': 'e1a4db5e-ac09-4058-b67a-f93ba6ca6bfc',\n",
       "   'Error': NameError(\"name 'create_pandas_dataframe_agent' is not defined\"),\n",
       "   'reference': {'reference_code': \"df['Survived'].sum()\"}},\n",
       "  '8bf9e5c9-bf6c-4b91-bf0c-8498073d91c9': {'input': {'question': 'How many passengers were on the Titanic?'},\n",
       "   'feedback': [],\n",
       "   'execution_time': 0.012868,\n",
       "   'run_id': '50d3d0ae-bce4-4caa-b502-59c7105c2990',\n",
       "   'Error': NameError(\"name 'create_pandas_dataframe_agent' is not defined\"),\n",
       "   'reference': {'reference_code': 'len(df)'}},\n",
       "  'a36f07a4-9b8c-43ff-9fc7-1caae8378fb9': {'input': {'question': 'How many male and female passengers were there?'},\n",
       "   'feedback': [],\n",
       "   'execution_time': 0.009967,\n",
       "   'run_id': 'dd056419-db07-4fbb-a766-5b11f620b9d1',\n",
       "   'Error': NameError(\"name 'create_pandas_dataframe_agent' is not defined\"),\n",
       "   'reference': {'reference_code': \"df['Sex'].value_counts()\"}},\n",
       "  'ae57634f-a538-4fdd-9535-46492887a870': {'input': {'question': 'What was the average fare paid for the tickets?'},\n",
       "   'feedback': [],\n",
       "   'execution_time': 0.012966,\n",
       "   'run_id': '880b5791-dacd-4150-8060-14523d3a4c36',\n",
       "   'Error': NameError(\"name 'create_pandas_dataframe_agent' is not defined\"),\n",
       "   'reference': {'reference_code': \"df['Fare'].mean()\"}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of our custom evaluator chain\n",
    "base_evaluator = DynamicReferenceEvaluator.from_llm(\n",
    "    criteria=\"correctness\", llm=ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    ")\n",
    "\n",
    "# Wrap it in a LangChainStringEvaluator to map the run/example fields correctly\n",
    "dynamic_evaluator = LangChainStringEvaluator(\n",
    "    base_evaluator,\n",
    "    # This function maps the dataset fields to what our evaluator expects\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"output\"],\n",
    "        \"reference\": example.outputs[\"reference_code\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ").as_run_evaluator()\n",
    "\n",
    "# Run the evaluation at Time \"T1\"\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=predict_pandas_agent,\n",
    "    evaluation=RunEvalConfig(\n",
    "        custom_evaluators=[dynamic_evaluator],\n",
    "    ),\n",
    "    project_metadata={\"time\": \"T1\"},\n",
    "    max_concurrency=1, # Pandas agent isn't thread-safe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a7553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
